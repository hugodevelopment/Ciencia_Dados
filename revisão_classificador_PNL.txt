Já está na parte final. negative_text = negative.iloc[0:1, [2,3]] Aqui ele usa um iloc para selecionar a primeira linhas, 0:1 porém 1 não é incluido e [2.3] para selecionar a 2 e 3 coluna do dataframe negative.


Já aqui ele cria uma variável categoria para selecionar apenas a informação variation
X_cat = negative_text[['variation']]
X_cat


Ele transforma em dummy variaveis para a aplicação do modelo e cria um dataframe a partir disso
X_cat = onehot.transform(X_cat).toarray()
X_cat = pd.DataFrame(X_cat)
X_cat

Para concatenar é importante que ambos os dataframes tenham o mesmo índice, senão acabará dando erro
X_cat.index = negative_text.index

Já aqui ele concatena ambos dataframes
negative_df = pd.concat([negative_text, X_cat], axis = 1)
negative_df

Aqui ele descartar a coluna variation considerando que apenas as colunas numericas serão utilizadas
negative_df.drop(['variation'], axis = 1, inplace = True)
negative_df

Aqui ele transformar os textos presentes na coluna em informações numericas utilizando a tokenização
negative_coutvectorizer = vec.transform(negative_df['verified_reviews'])

aqui ele converte em array
review = pd.DataFrame(negative_coutvectorizer.toarray())
review

Agora ele concatena apenas os dados numericos, sem dados cateorigos
negative_final = pd.concat([negative_df, review], axis = 1)
negative_final

agora ele utiliza o bayes predict no dataframe negative_final
bayes.predict(negative_final)

O resultado é um array negativo de classe 0, logo ele foi capaz de predizer corretamente que era um feedback negativo
->array([0])

agora ele está analisando a probabilidade de ser negativo ou positivo
bayes.predict_proba(negative_final)

Como resultado temos 0.73 de ser um feedback negativo e 0.26 de ser um feedback positivo
-> array([[0.73190851, 0.26809149]])

-----------------------------------------------------------------------------------------------------------*----------------------------------------------------------------------------------------

Para facilitar e deixar o codigo mais utilizável e organizado podemos criar uma função onde receberemos 2 paramentros, nesse caso são as colunas [variation e verified_reviews]

def_predict_negative(coluna1, coluna2):
	import pickle

	with open('text_classifier.pkl', 'rb') as f:
  	bayes, onehot, vec = pickle.load(f)
        
        negative_text = negative.iloc[0:1, [coluna1, coluna2]]

	X_cat = negative_text[['coluna1']]
	X_cat

       	X_cat = onehot.transform(X_cat).toarray()
	X_cat = pd.DataFrame(X_cat)
	X_cat

	negative_df = pd.concat([negative_text, X_cat], axis = 1)

	negative_df.drop(['coluna1'], axis = 1, inplace = True)
	negative_df

	negative_coutvectorizer = vec.transform(negative_df['coluna2'])

	review = pd.DataFrame(negative_coutvectorizer.toarray())

	negative_df.drop(['coluna 2'], axis = 1, inplace = True)

	negative_final = pd.concat([negative_df, review], axis = 1)

	bayes.predict(negative_final)

        print(bayes.predict_proba(negative_final))

-----------------------------------------------------------------------------------------------------------*----------------------------------------------------------------------------------------

Sim, a escolha entre técnicas como One-Hot Encoding e Tokenização (ou outras técnicas de vetorização) depende do tipo de dado que você está lidando.

One-Hot Encoding: É frequentemente usado para lidar com variáveis categóricas, onde você tem um conjunto discreto de categorias. A técnica cria colunas binárias (0 ou 1) para cada categoria única presente na coluna original. No caso da coluna "variation", pode-se assumir que são palavras únicas ou um número limitado de palavras, e cada palavra recebe sua própria coluna binária.

Tokenização: É mais apropriada para lidar com texto, especialmente quando você precisa representar palavras em frases ou documentos de maneira numérica. Cada palavra é transformada em um número ou vetor, e a representação final é uma matriz de números. A coluna "verified_reviews" provavelmente contém frases ou texto mais longo, por isso a tokenização é mais adequada.

Em resumo, a escolha entre essas técnicas depende da natureza dos dados em cada coluna. Se você tem variáveis categóricas, o One-Hot Encoding é uma escolha comum, enquanto a tokenização é mais útil quando lidando com texto. Em projetos de aprendizado de máquina, muitas vezes você pode precisar de uma combinação de várias técnicas para lidar com diferentes tipos de dados no conjunto de recursos.

        




